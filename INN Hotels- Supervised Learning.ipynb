{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "AT5OogJVFbwu",
   "metadata": {
    "id": "AT5OogJVFbwu"
   },
   "source": [
    "# INN Hotels Project\n",
    "\n",
    "## Context\n",
    "\n",
    "A significant number of hotel bookings are called-off due to cancellations or no-shows. The typical reasons for cancellations include change of plans, scheduling conflicts, etc. This is often made easier by the option to do so free of charge or preferably at a low cost which is beneficial to hotel guests but it is a less desirable and possibly revenue-diminishing factor for hotels to deal with. Such losses are particularly high on last-minute cancellations. \n",
    "\n",
    "The new technologies involving online booking channels have dramatically changed customers’ booking possibilities and behavior. This adds a further dimension to the challenge of how hotels handle cancellations, which are no longer limited to traditional booking and guest characteristics. \n",
    "\n",
    "The cancellation of bookings impact a hotel on various fronts:\n",
    "* Loss of resources (revenue) when the hotel cannot resell the room.\n",
    "* Additional costs of distribution channels by increasing commissions or paying for publicity to help sell these rooms.\n",
    "* Lowering prices last minute, so the hotel can resell a room, resulting in reducing the profit margin.\n",
    "* Human resources to make arrangements for the guests.\n",
    "\n",
    "## Objective\n",
    "The increasing number of cancellations calls for a Machine Learning based solution that can help in predicting which booking is likely to be canceled. INN Hotels Group has a chain of hotels in Portugal, they are facing problems with the high number of booking cancellations and have reached out to your firm for data-driven solutions. You as a data scientist have to analyze the data provided to find which factors have a high influence on booking cancellations, build a predictive model that can predict which booking is going to be canceled in advance, and help in formulating profitable policies for cancellations and refunds.\n",
    "\n",
    "## Data Description\n",
    "The data contains the different attributes of customers' booking details. The detailed data dictionary is given below.\n",
    "\n",
    "\n",
    "**Data Dictionary**\n",
    "\n",
    "* Booking_ID: unique identifier of each booking\n",
    "* no_of_adults: Number of adults\n",
    "* no_of_children: Number of Children\n",
    "* no_of_weekend_nights: Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel\n",
    "* no_of_week_nights: Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel\n",
    "* type_of_meal_plan: Type of meal plan booked by the customer:\n",
    "    * Not Selected – No meal plan selected\n",
    "    * Meal Plan 1 – Breakfast\n",
    "    * Meal Plan 2 – Half board (breakfast and one other meal)\n",
    "    * Meal Plan 3 – Full board (breakfast, lunch, and dinner)\n",
    "* required_car_parking_space: Does the customer require a car parking space? (0 - No, 1- Yes)\n",
    "* room_type_reserved: Type of room reserved by the customer. The values are ciphered (encoded) by INN Hotels.\n",
    "* lead_time: Number of days between the date of booking and the arrival date\n",
    "* arrival_year: Year of arrival date\n",
    "* arrival_month: Month of arrival date\n",
    "* arrival_date: Date of the month\n",
    "* market_segment_type: Market segment designation.\n",
    "* repeated_guest: Is the customer a repeated guest? (0 - No, 1- Yes)\n",
    "* no_of_previous_cancellations: Number of previous bookings that were canceled by the customer prior to the current booking\n",
    "* no_of_previous_bookings_not_canceled: Number of previous bookings not canceled by the customer prior to the current booking\n",
    "* avg_price_per_room: Average price per day of the reservation; prices of the rooms are dynamic. (in euros)\n",
    "* no_of_special_requests: Total number of special requests made by the customer (e.g. high floor, view from the room, etc)\n",
    "* booking_status: Flag indicating if the booking was canceled or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-island",
   "metadata": {
    "id": "dirty-island"
   },
   "source": [
    "## Importing necessary libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-still",
   "metadata": {
    "id": "statewide-still"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.simplefilter(\"ignore\", ConvergenceWarning)\n",
    "\n",
    "# Libraries to help with reading and manipulating data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# libaries to help with data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Removes the limit for the number of displayed columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# Sets the limit for the number of displayed rows\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "# setting the precision of floating numbers to 5 decimal points\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.5f\" % x)\n",
    "\n",
    "# Library to split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# To build model for prediction\n",
    "import statsmodels.stats.api as sms\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "# To tune different models\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# To get diferent metric scores\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    plot_confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    make_scorer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-infection",
   "metadata": {
    "id": "desperate-infection"
   },
   "source": [
    "## Data Overview\n",
    "\n",
    "- Observations\n",
    "- Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-juice",
   "metadata": {
    "id": "persistent-juice"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('INNHotelsGroup.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4480bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb38f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc11244",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bcb249",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e875e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4162e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c08eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc93fb89",
   "metadata": {},
   "source": [
    "- No missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a29f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Booking_ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572aba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-calibration",
   "metadata": {
    "id": "seasonal-calibration"
   },
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "- EDA is an important part of any project involving data.\n",
    "- It is important to investigate and understand the data better before building a model with it.\n",
    "- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.\n",
    "- A thorough analysis of the data, in addition to the questions mentioned below, should be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-traveler",
   "metadata": {
    "id": "classified-traveler"
   },
   "source": [
    "**Leading Questions**:\n",
    "1. What are the busiest months in the hotel?\n",
    "2. Which market segment do most of the guests come from?\n",
    "3. Hotel rates are dynamic and change according to demand and customer demographics. What are the differences in room prices in different market segments?\n",
    "4. What percentage of bookings are canceled? \n",
    "5. Repeating guests are the guests who stay in the hotel often and are important to brand equity. What percentage of repeating guests cancel?\n",
    "6. Many guests have special requirements when booking a hotel room. Do these requirements affect booking cancellation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d41f01",
   "metadata": {},
   "source": [
    "**The below functions need to be defined to carry out the EDA.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-interference",
   "metadata": {
    "id": "mechanical-interference"
   },
   "outputs": [],
   "source": [
    "def histogram_boxplot(data, feature, figsize=(15, 10), kde=False, bins=None):\n",
    "    \"\"\"\n",
    "    Boxplot and histogram combined\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    figsize: size of figure (default (15,10))\n",
    "    kde: whether to show the density curve (default False)\n",
    "    bins: number of bins for histogram (default None)\n",
    "    \"\"\"\n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
    "        nrows=2,  # Number of rows of the subplot grid= 2\n",
    "        sharex=True,  # x-axis will be shared among all subplots\n",
    "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
    "        figsize=figsize,\n",
    "    )  # creating the 2 subplots\n",
    "    sns.boxplot(\n",
    "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
    "    )  # boxplot will be created and a triangle will indicate the mean value of the column\n",
    "    sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins\n",
    "    ) if bins else sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
    "    )  # For histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
    "    )  # Add mean to the histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
    "    )  # Add median to the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960d7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create labeled barplots\n",
    "\n",
    "\n",
    "def labeled_barplot(data, feature, perc=False, n=None):\n",
    "    \"\"\"\n",
    "    Barplot with percentage at the top\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    perc: whether to display percentages instead of count (default is False)\n",
    "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
    "    \"\"\"\n",
    "\n",
    "    total = len(data[feature])  # length of the column\n",
    "    count = data[feature].nunique()\n",
    "    if n is None:\n",
    "        plt.figure(figsize=(count + 2, 6))\n",
    "    else:\n",
    "        plt.figure(figsize=(n + 2, 6))\n",
    "\n",
    "    plt.xticks(rotation=90, fontsize=15)\n",
    "    ax = sns.countplot(\n",
    "        data=data,\n",
    "        x=feature,\n",
    "        palette=\"Paired\",\n",
    "        order=data[feature].value_counts().index[:n],\n",
    "    )\n",
    "\n",
    "    for p in ax.patches:\n",
    "        if perc == True:\n",
    "            label = \"{:.1f}%\".format(\n",
    "                100 * p.get_height() / total\n",
    "            )  # percentage of each class of the category\n",
    "        else:\n",
    "            label = p.get_height()  # count of each level of the category\n",
    "\n",
    "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
    "        y = p.get_height()  # height of the plot\n",
    "\n",
    "        ax.annotate(\n",
    "            label,\n",
    "            (x, y),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            size=12,\n",
    "            xytext=(0, 5),\n",
    "            textcoords=\"offset points\",\n",
    "        )  # annotate the percentage\n",
    "\n",
    "    plt.show()  # show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_barplot(data, predictor, target):\n",
    "    \"\"\"\n",
    "    Print the category counts and plot a stacked bar chart\n",
    "\n",
    "    data: dataframe\n",
    "    predictor: independent variable\n",
    "    target: target variable\n",
    "    \"\"\"\n",
    "    count = data[predictor].nunique()\n",
    "    sorter = data[target].value_counts().index[-1]\n",
    "    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n",
    "        by=sorter, ascending=False\n",
    "    )\n",
    "    print(tab1)\n",
    "    print(\"-\" * 120)\n",
    "    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n",
    "        by=sorter, ascending=False\n",
    "    )\n",
    "    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 5, 5))\n",
    "    plt.legend(\n",
    "        loc=\"lower left\", frameon=False,\n",
    "    )\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2f88ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to plot distributions wrt target\n",
    "\n",
    "\n",
    "def distribution_plot_wrt_target(data, predictor, target):\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    target_uniq = data[target].unique()\n",
    "\n",
    "    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n",
    "    sns.histplot(\n",
    "        data=data[data[target] == target_uniq[0]],\n",
    "        x=predictor,\n",
    "        kde=True,\n",
    "        ax=axs[0, 0],\n",
    "        color=\"teal\",\n",
    "        stat=\"density\",\n",
    "    )\n",
    "\n",
    "    axs[0, 1].set_title(\"Distribution of target for target=\" + str(target_uniq[1]))\n",
    "    sns.histplot(\n",
    "        data=data[data[target] == target_uniq[1]],\n",
    "        x=predictor,\n",
    "        kde=True,\n",
    "        ax=axs[0, 1],\n",
    "        color=\"orange\",\n",
    "        stat=\"density\",\n",
    "    )\n",
    "\n",
    "    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n",
    "    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n",
    "\n",
    "    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n",
    "    sns.boxplot(\n",
    "        data=data,\n",
    "        x=target,\n",
    "        y=predictor,\n",
    "        ax=axs[1, 1],\n",
    "        showfliers=False,\n",
    "        palette=\"gist_rainbow\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450502e4",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f618f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of bookings in each month\n",
    "month_counts = data['arrival_month'].value_counts()\n",
    "\n",
    "# Sort the months in chronological order (assuming the months are represented as numbers)\n",
    "sorted_months = sorted(month_counts.index)\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(sorted_months, month_counts[sorted_months])\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Bookings')\n",
    "plt.title('Number of Bookings per Month')\n",
    "plt.xticks(sorted_months)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1be314",
   "metadata": {},
   "source": [
    "- October is the busiest month and January is slowest month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02571230",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_counts = data['market_segment_type'].value_counts()\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(segment_counts.index, segment_counts)\n",
    "plt.xlabel('Market Segment')\n",
    "plt.ylabel('Number of Bookings')\n",
    "plt.title('Number of Bookings per Market Segment')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb06264",
   "metadata": {},
   "source": [
    "- Most Bookings come from online market segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadeafc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(data, \"avg_price_per_room\", bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130fc1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"avg_price_per_room\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001ce78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"avg_price_per_room\"] > 0, \"market_segment_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8553b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"avg_price_per_room\"] == 0, \"market_segment_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e18f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='market_segment_type', y='avg_price_per_room', data=data)\n",
    "plt.xlabel('Market Segment')\n",
    "plt.ylabel('Average Room Price')\n",
    "plt.title('Room Price Variation Across Market Segments')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88314677",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(data, \"no_of_previous_cancellations\", bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878af22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(data, \"no_of_previous_bookings_not_canceled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abfbc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(data, \"no_of_adults\",perc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(data, \"no_of_children\",perc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5b4d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(data, \"no_of_weekend_nights\",perc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6c0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(data, \"no_of_week_nights\",perc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc99c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(data, \"type_of_meal_plan\",perc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d840ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(data, \"required_car_parking_space\",perc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cfe984",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(data, \"no_of_special_requests\",perc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7bc668",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(data, \"room_type_reserved\",perc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031d9a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(data, \"market_segment_type\",perc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fecfb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(data, \"booking_status\",perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a944b9",
   "metadata": {},
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48773d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"booking_status\"] = data[\"booking_status\"].apply(\n",
    "    lambda x: 1 if x == \"Canceled\" else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c7396",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_list = data.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.heatmap(\n",
    "    data[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f92b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(\n",
    "    data=data, x=\"market_segment_type\", y=\"avg_price_per_room\", palette=\"gist_rainbow\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f494fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_barplot(data, \"no_of_special_requests\", \"booking_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe38e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_barplot(data, \"market_segment_type\", \"booking_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b83783",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_barplot(data, \"repeated_guest\", \"booking_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1745f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_plot_wrt_target(data, \"lead_time\", \"booking_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9526419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_plot_wrt_target(data, \"avg_price_per_room\", \"booking_status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-spirituality",
   "metadata": {
    "id": "alleged-spirituality"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "- Missing value treatment (if needed)\n",
    "- Feature engineering (if needed)\n",
    "- Outlier detection and treatment (if needed)\n",
    "- Preparing data for modeling \n",
    "- Any other preprocessing steps (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1969fa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-louisiana",
   "metadata": {
    "id": "increasing-louisiana",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# outlier detection using boxplot\n",
    "numeric_columns = data.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "for i, variable in enumerate(numeric_columns):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.boxplot(data[variable], whis=1.5)\n",
    "    plt.tight_layout()\n",
    "    plt.title(variable)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af4ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"booking_status\"], axis=1)\n",
    "Y = data[\"booking_status\"]\n",
    "\n",
    "# Adding constant to X\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Creating dummies for X\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c7a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of Training set : \", X_train.shape)\n",
    "print(\"Shape of test set : \", X_test.shape)\n",
    "print(\"Percentage of classes in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"Percentage of classes in test set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9754c766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad51b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37114f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc82e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1cebb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420baeb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d9aac9a",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71131df0",
   "metadata": {
    "id": "interested-talent"
   },
   "source": [
    "- It is a good idea to explore the data once again after manipulating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1622b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5126425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(300,200))\n",
    "sns.pairplot(data[[\"no_of_adults\", \"no_of_children\", \"no_of_weekend_nights\", \"lead_time\", \"arrival_year\",'arrival_date','arrival_month','repeated_guest','no_of_previous_cancellations','no_of_previous_bookings_not_canceled','avg_price_per_room','no_of_special_requests']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-projection",
   "metadata": {
    "id": "third-projection"
   },
   "source": [
    "## Checking Multicollinearity\n",
    "\n",
    "- In order to make statistical inferences from a logistic regression model, it is important to ensure that there is no multicollinearity present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-payroll",
   "metadata": {
    "id": "antique-payroll"
   },
   "outputs": [],
   "source": [
    "# defining a function to compute different metrics to check performance of a classification model built using statsmodels\n",
    "def model_performance_classification_statsmodels(\n",
    "    model, predictors, target, threshold=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to compute different metrics to check classification model performance\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    threshold: threshold for classifying the observation as class 1\n",
    "    \"\"\"\n",
    "\n",
    "    # checking which probabilities are greater than threshold\n",
    "    pred_temp = model.predict(predictors) > threshold\n",
    "    # rounding off the above values to get classes\n",
    "    pred = np.round(pred_temp)\n",
    "\n",
    "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
    "    recall = recall_score(target, pred)  # to compute Recall\n",
    "    precision = precision_score(target, pred)  # to compute Precision\n",
    "    f1 = f1_score(target, pred)  # to compute F1-score\n",
    "\n",
    "    # creating a dataframe of metrics\n",
    "    data_perf = pd.DataFrame(\n",
    "        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1\": f1,},\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    return data_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to plot the confusion_matrix of a classification model\n",
    "\n",
    "\n",
    "def confusion_matrix_statsmodels(model, predictors, target, threshold=0.5):\n",
    "    \"\"\"\n",
    "    To plot the confusion_matrix with percentages\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    threshold: threshold for classifying the observation as class 1\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(predictors) > threshold\n",
    "    cm = confusion_matrix(target, y_pred)\n",
    "    labels = np.asarray(\n",
    "        [\n",
    "            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n",
    "            for item in cm.flatten()\n",
    "        ]\n",
    "    ).reshape(2, 2)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=labels, fmt=\"\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = sm.Logit(y_train, X_train.astype(float))\n",
    "lg = logit.fit(disp=False)\n",
    "\n",
    "print(lg.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c4bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_statsmodels(lg, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9640605",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training performance:\")\n",
    "model_performance_classification_statsmodels(lg, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "def checking_vif(predictors):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"feature\"] = predictors.columns\n",
    "\n",
    "    # calculating VIF for each feature\n",
    "    vif[\"VIF\"] = [\n",
    "        variance_inflation_factor(predictors.values, i)\n",
    "        for i in range(len(predictors.columns))\n",
    "    ]\n",
    "    return vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613d1be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_vif(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac611148",
   "metadata": {},
   "source": [
    "- Variables with VIF values close to 1 indicate minimal multicollinearity, signifying their independence from each other.\n",
    "- Even slightly elevated VIF values above 1, like for required_car_parking_space, lead_time, and arrival_year, still indicate low multicollinearity and pose no significant issue.\n",
    "- Variables with VIF values between 1 and 5, such as arrival_month, arrival_date, and no_of_special_requests, show acceptable levels of multicollinearity, while those exceeding 5, like avg_price_per_room, require closer scrutiny for potential impact on model stability and interpretation. Higher VIF values for market segment variables may raise concerns about their effect on model reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-iceland",
   "metadata": {
    "id": "domestic-iceland"
   },
   "source": [
    "## Building a Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87eaa59",
   "metadata": {},
   "source": [
    "### Removing high p-value variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7f83fe",
   "metadata": {},
   "source": [
    "* For other attributes present in the data, the p-values are high only for few dummy variables and since only one (or some) of the categorical levels have a high p-value we will drop them iteratively as sometimes p-values change after dropping a variable. So, we'll not drop all variables at once.\n",
    "\n",
    "* Instead, we will do the following repeatedly using a loop:\n",
    "  - Build a model, check the p-values of the variables, and drop the column with the highest p-value.\n",
    "  - Create a new model without the dropped feature, check the p-values of the variables, and drop the column with the highest p-value.\n",
    "  - Repeat the above two steps till there are no columns with p-value > 0.05.\n",
    "\n",
    "\n",
    "Note: The above process can also be done manually by picking one variable at a time that has a high p-value, dropping it, and building a model again. But that might be a little tedious and using a loop will be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-institution",
   "metadata": {
    "id": "unknown-institution"
   },
   "outputs": [],
   "source": [
    "# initial list of columns\n",
    "cols = X_train.columns.tolist()\n",
    "\n",
    "# setting an initial max p-value\n",
    "max_p_value = 1\n",
    "\n",
    "while len(cols) > 0:\n",
    "    # defining the train set\n",
    "    X_train_aux = X_train[cols]\n",
    "\n",
    "    # fitting the model\n",
    "    model = sm.Logit(y_train, X_train_aux).fit(disp=False)\n",
    "\n",
    "    # getting the p-values and the maximum p-value\n",
    "    p_values = model.pvalues\n",
    "    max_p_value = max(p_values)\n",
    "\n",
    "    # name of the variable with maximum p-value\n",
    "    feature_with_p_max = p_values.idxmax()\n",
    "\n",
    "    if max_p_value > 0.05:\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "selected_features = cols\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train[selected_features]\n",
    "X_test1 = X_test[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da034ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit2 = sm.Logit(y_train, X_train1.astype(float))\n",
    "lg2 = logit2.fit(disp=False)\n",
    "\n",
    "print(lg2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b37f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training performance:\")\n",
    "model_performance_classification_statsmodels(lg2, X_train1, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a0ea35",
   "metadata": {},
   "source": [
    "**Now no categorical feature has p-value greater than 0.05, so we'll consider the features in *X_train1* as the final ones and *lg2* as final model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03c19e6",
   "metadata": {},
   "source": [
    "**Converting coefficients to odds**\n",
    "\n",
    "* The coefficients ($\\beta$s) of the logistic regression model are in terms of $log(odds)$ and to find the odds, we have to take the exponential of the coefficients\n",
    "* Therefore, **$odds =  exp(\\beta)$**\n",
    "* The percentage change in odds is given as $(exp(\\beta) - 1) * 100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16321406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting coefficients to odds\n",
    "odds = np.exp(lg2.params)\n",
    "\n",
    "# finding the percentage change\n",
    "perc_change_odds = (np.exp(lg2.params) - 1) * 100\n",
    "\n",
    "# removing limit from number of columns to display\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# adding the odds to a dataframe\n",
    "pd.DataFrame({\"Odds\": odds, \"Change_odd%\": perc_change_odds}, index=X_train1.columns).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a075b37a",
   "metadata": {},
   "source": [
    "- Among the numeric variables, 'no_of_adults' and 'no_of_children' have moderate positive impacts on the odds, with respective change percentages of 11.48% and 16.44%.\n",
    "\n",
    "- Variables like 'required_car_parking_space' and 'lead_time' exhibit substantial negative impacts on the odds, reducing them by approximately 79.70% and 83.25%, respectively.\n",
    "\n",
    "- Categorical variables, such as 'type_of_meal_plan_Not Selected' and 'room_type_reserved_Room_Type 7', have notable negative effects on the odds, reducing them by around 76.10% and 54.74%, respectively.\n",
    "\n",
    "- Variables like 'arrival_month' and 'arrival_year' have relatively modest impacts on the odds, with a change percentage of around 4.15% and 57.32%, respectively.\n",
    "\n",
    "- It's important to interpret these odds ratios and changes in odds percentages in the context of your specific analysis and domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-season",
   "metadata": {
    "id": "historic-season"
   },
   "source": [
    "## Model performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cd22a4",
   "metadata": {},
   "source": [
    "**Training set performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-conservation",
   "metadata": {
    "id": "integral-conservation"
   },
   "outputs": [],
   "source": [
    "confusion_matrix_statsmodels(lg2, X_train1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db297ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_train_perf = model_performance_classification_statsmodels(\n",
    "    lg2, X_train1, y_train\n",
    ")\n",
    "\n",
    "print(\"Training performance:\")\n",
    "log_reg_model_train_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43405822",
   "metadata": {},
   "source": [
    "**Test set performance**\n",
    "\n",
    "- We have to first drop the columns from the test set that were dropped from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a2d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1 = X_test[list(X_train1.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66bd5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_statsmodels(lg2, X_test1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd81d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_test_perf = model_performance_classification_statsmodels(\n",
    "    lg2, X_test1, y_test\n",
    ")\n",
    "\n",
    "print(\"Test performance:\")\n",
    "log_reg_model_test_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal threshold as per AUC-ROC curve\n",
    "# The optimal cut off would be where tpr is high and fpr is low\n",
    "fpr, tpr, thresholds = roc_curve(y_train, lg2.predict(X_train1))\n",
    "\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold_auc_roc = thresholds[optimal_idx]\n",
    "print(optimal_threshold_auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398fb95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_roc_auc_train = roc_auc_score(y_train, lg2.predict(X_train1))\n",
    "fpr, tpr, thresholds = roc_curve(y_train, lg2.predict(X_train1))\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, label=\"Logistic Regression (area = %0.2f)\" % logit_roc_auc_train)\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.01])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297fa78c",
   "metadata": {},
   "source": [
    "* Logistic Regression model is giving a good performance on training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff77cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix\n",
    "confusion_matrix_statsmodels(\n",
    "    lg2, X_train1, y_train, threshold=optimal_threshold_auc_roc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c13666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking model performance for this model\n",
    "log_reg_model_train_perf_threshold_auc_roc = model_performance_classification_statsmodels(\n",
    "    lg2, X_train1, y_train, threshold=optimal_threshold_auc_roc\n",
    ")\n",
    "print(\"Training performance:\")\n",
    "log_reg_model_train_perf_threshold_auc_roc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684ce3a8",
   "metadata": {},
   "source": [
    "- The model is still giving a good performance.\n",
    "- Recall and F1 score increased , but precision and accuracy dropped slightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d7ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_roc_auc_train = roc_auc_score(y_test, lg2.predict(X_test1))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, lg2.predict(X_test1))\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, label=\"Logistic Regression (area = %0.2f)\" % logit_roc_auc_train)\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05fa711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix\n",
    "confusion_matrix_statsmodels(lg2, X_test1, y_test, threshold=optimal_threshold_auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a46f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking model performance for this model\n",
    "log_reg_model_test_perf_threshold_auc_roc = model_performance_classification_statsmodels(\n",
    "    lg2, X_test1, y_test, threshold=optimal_threshold_auc_roc\n",
    ")\n",
    "print(\"Test performance:\")\n",
    "log_reg_model_test_perf_threshold_auc_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb1a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = lg2.predict(X_train1)\n",
    "prec, rec, tre = precision_recall_curve(y_train, y_scores,)\n",
    "\n",
    "\n",
    "def plot_prec_recall_vs_tresh(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g--\", label=\"recall\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plot_prec_recall_vs_tresh(prec, rec, tre)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db831d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_threshold_curve = 0.42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1512254",
   "metadata": {},
   "source": [
    "* At the threshold of 0.42, we get balanced recall and precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1403cc4f",
   "metadata": {},
   "source": [
    "**Checking model performance on training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288dd1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix\n",
    "confusion_matrix_statsmodels(lg2, X_train1, y_train, threshold=optimal_threshold_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc36859",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_train_perf_threshold_curve = model_performance_classification_statsmodels(\n",
    "    lg2, X_train1, y_train, threshold=optimal_threshold_curve\n",
    ")\n",
    "print(\"Training performance:\")\n",
    "log_reg_model_train_perf_threshold_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c8fe2f",
   "metadata": {},
   "source": [
    "**Checking model performance on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f9c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    " # creating confusion matrix\n",
    "confusion_matrix_statsmodels(lg2, X_test1, y_test, threshold=optimal_threshold_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d11b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_test_perf_threshold_curve = model_performance_classification_statsmodels(\n",
    "    lg2, X_test1, y_test, threshold=optimal_threshold_curve\n",
    ")\n",
    "print(\"Test performance:\")\n",
    "log_reg_model_test_perf_threshold_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-collaboration",
   "metadata": {
    "id": "stylish-collaboration"
   },
   "source": [
    "## Final Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-breathing",
   "metadata": {
    "id": "related-breathing"
   },
   "outputs": [],
   "source": [
    "# training performance comparison\n",
    "\n",
    "models_train_comp_df = pd.concat(\n",
    "    [\n",
    "        log_reg_model_train_perf.T,\n",
    "        log_reg_model_train_perf_threshold_auc_roc.T,\n",
    "        log_reg_model_train_perf_threshold_curve.T,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "models_train_comp_df.columns = [\n",
    "    \"Logistic Regression-default Threshold\",\n",
    "    \"Logistic Regression-0.35 Threshold\",\n",
    "    \"Logistic Regression-0.43 Threshold\",\n",
    "]\n",
    "\n",
    "print(\"Training performance comparison:\")\n",
    "models_train_comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b426b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training performance comparison\n",
    "\n",
    "models_test_comp_df = pd.concat(\n",
    "    [\n",
    "        log_reg_model_test_perf.T,\n",
    "        log_reg_model_test_perf_threshold_auc_roc.T,\n",
    "        log_reg_model_test_perf_threshold_curve.T,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "models_test_comp_df.columns = [\n",
    "    \"Logistic Regression-default Threshold\",\n",
    "    \"Logistic Regression-0.35 Threshold\",\n",
    "    \"Logistic Regression-0.43 Threshold\",\n",
    "]\n",
    "\n",
    "print(\"Test performance comparison:\")\n",
    "models_test_comp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3353a2",
   "metadata": {},
   "source": [
    "- Almost all the three models are performing well on both training and test data without the problem of overfitting\n",
    "- The model with a default threshold (0.5) is giving the best overall score. Therefore it can be selected as the final model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-fluid",
   "metadata": {
    "id": "amazing-fluid"
   },
   "source": [
    "## Building a Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62697d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to compute different metrics to check performance of a classification model built using sklearn\n",
    "def model_performance_classification_sklearn(model, predictors, target):\n",
    "    \"\"\"\n",
    "    Function to compute different metrics to check classification model performance\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "\n",
    "    # predicting using the independent variables\n",
    "    pred = model.predict(predictors)\n",
    "\n",
    "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
    "    recall = recall_score(target, pred)  # to compute Recall\n",
    "    precision = precision_score(target, pred)  # to compute Precision\n",
    "    f1 = f1_score(target, pred)  # to compute F1-score\n",
    "\n",
    "    # creating a dataframe of metrics\n",
    "    df_perf = pd.DataFrame(\n",
    "        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1\": f1,},\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    return df_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a95956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_sklearn(model, predictors, target):\n",
    "    \"\"\"\n",
    "    To plot the confusion_matrix with percentages\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(predictors)\n",
    "    cm = confusion_matrix(target, y_pred)\n",
    "    labels = np.asarray(\n",
    "        [\n",
    "            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n",
    "            for item in cm.flatten()\n",
    "        ]\n",
    "    ).reshape(2, 2)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=labels, fmt=\"\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd09d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"booking_status\"], axis=1)\n",
    "Y = data[\"booking_status\"]\n",
    "\n",
    "\n",
    "# Creating dummies for X\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-hydrogen",
   "metadata": {
    "id": "neither-hydrogen"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "model = DecisionTreeClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51215734",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sklearn(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6aee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_perf_train= model_performance_classification_sklearn(\n",
    "    model, X_train, y_train\n",
    ")\n",
    "decision_tree_perf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0cfa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sklearn(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d3504",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_perf_test = model_performance_classification_sklearn(\n",
    "    model, X_test, y_test\n",
    ")\n",
    "decision_tree_perf_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4787f158",
   "metadata": {},
   "source": [
    "- There is a huge disparity in performance of model on training set and test set, which suggests that the model is overfiiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9367cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(X_train.columns)\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd007e",
   "metadata": {},
   "source": [
    "- In the pre-tuned Tree Lead Time and Average price per room are the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-strengthening",
   "metadata": {
    "id": "limited-strengthening"
   },
   "source": [
    "## Do we need to prune the tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7ea99",
   "metadata": {
    "id": "ambient-elements"
   },
   "source": [
    "### Decision Tree (Pre-pruning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f44fdc1",
   "metadata": {},
   "source": [
    "**Using GridSearch for Hyperparameter tuning of our tree model**\n",
    "\n",
    "* Hyperparameter tuning is also tricky in the sense that there is no direct way to calculate how a change in the\n",
    "  hyperparameter value will reduce the loss of your model, so we usually resort to experimentation. i.e we'll use Grid search\n",
    "* Grid search is a tuning technique that attempts to compute the optimum values of hyperparameters. \n",
    "* It is an exhaustive search that is performed on a the specific parameter values of a model.\n",
    "* The parameters of the estimator/model used to apply these methods are optimized by cross-validated grid-search over a parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the type of classifier.\n",
    "estimator = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "parameters = {\n",
    "    \"max_depth\": np.arange(2, 7, 2),\n",
    "    \"max_leaf_nodes\": [50, 75, 150, 250],\n",
    "    \"min_samples_split\": [10, 30, 50, 70],\n",
    "}\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "acc_scorer = metrics.make_scorer(metrics.recall_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "estimator = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data.\n",
    "estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4846c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall_score(model):\n",
    "    '''\n",
    "    model : classifier to predict values of X\n",
    "\n",
    "    '''\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    print(\"Recall on training set : \",metrics.recall_score(y_train,pred_train))\n",
    "    print(\"Recall on test set : \",metrics.recall_score(y_test,pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ab4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = DecisionTreeClassifier(criterion = 'gini',max_depth=3,random_state=1)\n",
    "model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92c5369",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sklearn(estimator, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae29f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_tune_perf_train = model_performance_classification_sklearn(\n",
    "    estimator, X_train, y_train\n",
    ")\n",
    "decision_tree_tune_perf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03385eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sklearn(estimator, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a1542",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_tune_perf_test = model_performance_classification_sklearn(\n",
    "    estimator, X_test, y_test\n",
    ")\n",
    "decision_tree_tune_perf_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d40c8e",
   "metadata": {},
   "source": [
    "* The model is giving a generalized result now since the recall scores on both the train and test data are coming to be around 0.70 which shows that the model is able to generalize well on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b80c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training set : \",model1.score(X_train, y_train))\n",
    "print(\"Accuracy on test set : \",model1.score(X_test, y_test))\n",
    "# Recall on train and test\n",
    "get_recall_score(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dea76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(X_train.columns)\n",
    "importances = estimator.feature_importances_\n",
    "indices = np.argsort(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfdb92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "out = tree.plot_tree(\n",
    "    estimator,\n",
    "    feature_names=feature_names,\n",
    "    filled=True,\n",
    "    fontsize=9,\n",
    "    node_ids=False,\n",
    "    class_names=None,\n",
    ")\n",
    "# below code will add arrows to the decision tree split if they are missing\n",
    "for o in out:\n",
    "    arrow = o.arrow_patch\n",
    "    if arrow is not None:\n",
    "        arrow.set_edgecolor(\"black\")\n",
    "        arrow.set_linewidth(1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a985da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text report showing the rules of a decision tree -\n",
    "print(tree.export_text(estimator, feature_names=feature_names, show_weights=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c14fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance of features in the tree building\n",
    "\n",
    "importances = estimator.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c63a9d",
   "metadata": {},
   "source": [
    "- After Pre-Pruning we can say that Lead time,Market Segment type online and Average price per room are important features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32691701",
   "metadata": {},
   "source": [
    "### Cost Complexity Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e73dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=1, class_weight=\"balanced\")\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = abs(path.ccp_alphas), path.impurities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518cfa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b03abf4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64677a48",
   "metadata": {},
   "source": [
    "Next, we train a decision tree using the effective alphas. The last value\n",
    "in ``ccp_alphas`` is the alpha value that prunes the whole tree,\n",
    "leaving the tree, ``clfs[-1]``, with one node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf8fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(\n",
    "        random_state=1, ccp_alpha=ccp_alpha, class_weight=\"balanced\"\n",
    "    )\n",
    "    clf.fit(X_train, y_train)  # Fit decision tree on training data\n",
    "    clfs.append(clf)\n",
    "\n",
    "print(\n",
    "    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "        clfs[-1].tree_.node_count, ccp_alphas[-1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dea233",
   "metadata": {},
   "source": [
    "\n",
    "For the remainder, we remove the last element in\n",
    "``clfs`` and ``ccp_alphas``, because it is the trivial tree with only one\n",
    "node. Here we show that the number of nodes and tree depth decreases as alpha\n",
    "increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ea641",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = clfs[:-1]\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "node_counts = [clf.tree_.node_count for clf in clfs]\n",
    "depth = [clf.tree_.max_depth for clf in clfs]\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 7))\n",
    "ax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax[0].set_xlabel(\"alpha\")\n",
    "ax[0].set_ylabel(\"number of nodes\")\n",
    "ax[0].set_title(\"Number of nodes vs alpha\")\n",
    "ax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax[1].set_xlabel(\"alpha\")\n",
    "ax[1].set_ylabel(\"depth of tree\")\n",
    "ax[1].set_title(\"Depth vs alpha\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77187065",
   "metadata": {},
   "source": [
    "Accuracy vs alpha for training and testing sets\n",
    "----------------------------------------------------\n",
    "When ``ccp_alpha`` is set to zero and keeping the other default parameters\n",
    "of `DecisionTreeClassifier`, the tree overfits, leading to\n",
    "a 100% training accuracy and 69% testing accuracy. As alpha increases, more\n",
    "of the tree is pruned, thus creating a decision tree that generalizes better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e83db1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
    "test_scores = [clf.score(X_test, y_test) for clf in clfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84e52dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc965de",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_best_model = np.argmax(test_scores)\n",
    "best_model = clfs[index_best_model]\n",
    "print(best_model)\n",
    "print('Training accuracy of best model: ',best_model.score(X_train, y_train))\n",
    "print('Test accuracy of best model: ',best_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351ac48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_train = []\n",
    "for clf in clfs:\n",
    "    pred_train = clf.predict(X_train)\n",
    "    values_train = recall_score(y_train, pred_train)\n",
    "    recall_train.append(values_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_test = []\n",
    "for clf in clfs:\n",
    "    pred_test = clf.predict(X_test)\n",
    "    values_test = recall_score(y_test, pred_test)\n",
    "    recall_test.append(values_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7df6fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
    "test_scores = [clf.score(X_test, y_test) for clf in clfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2c5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"Recall\")\n",
    "ax.set_title(\"Recall vs alpha for training and testing sets\")\n",
    "ax.plot(\n",
    "    ccp_alphas, recall_train, marker=\"o\", label=\"train\", drawstyle=\"steps-post\",\n",
    ")\n",
    "ax.plot(ccp_alphas, recall_test, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63f8615",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_best_model = np.argmax(recall_test)\n",
    "best_model = clfs[index_best_model]\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a464340",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sklearn(best_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_post_perf_train_recall = model_performance_classification_sklearn(\n",
    "    best_model, X_train, y_train\n",
    ")\n",
    "decision_tree_post_perf_train_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035b5efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sklearn(best_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e1cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_post_test_recall = model_performance_classification_sklearn(\n",
    "    best_model, X_test, y_test\n",
    ")\n",
    "decision_tree_post_test_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a335772",
   "metadata": {},
   "source": [
    "#### Since accuracy isn't the right metric for our data we should also perform F1 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72444d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_train = []\n",
    "for clf in clfs:\n",
    "    pred_train = clf.predict(X_train)\n",
    "    values_train = f1_score(y_train, pred_train)\n",
    "    f1_train.append(values_train)\n",
    "\n",
    "f1_test = []\n",
    "for clf in clfs:\n",
    "    pred_test = clf.predict(X_test)\n",
    "    values_test = f1_score(y_test, pred_test)\n",
    "    f1_test.append(values_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797155b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"F1 Score\")\n",
    "ax.set_title(\"F1 Score vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, f1_train, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, f1_test, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_best_model = np.argmax(f1_test)\n",
    "best_model = clfs[index_best_model]\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034903f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sklearn(best_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1188d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_post_perf_train = model_performance_classification_sklearn(\n",
    "    best_model, X_train, y_train\n",
    ")\n",
    "decision_tree_post_perf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sklearn(best_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb644b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_post_test = model_performance_classification_sklearn(\n",
    "    best_model, X_test, y_test\n",
    ")\n",
    "decision_tree_post_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bea94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "out = tree.plot_tree(\n",
    "    best_model,\n",
    "    feature_names=feature_names,\n",
    "    filled=True,\n",
    "    fontsize=9,\n",
    "    node_ids=False,\n",
    "    class_names=None,\n",
    ")\n",
    "for o in out:\n",
    "    arrow = o.arrow_patch\n",
    "    if arrow is not None:\n",
    "        arrow.set_edgecolor(\"black\")\n",
    "        arrow.set_linewidth(1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772cd7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text report showing the rules of a decision tree -\n",
    "\n",
    "print(tree.export_text(best_model, feature_names=feature_names, show_weights=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de7a981",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = best_model.feature_importances_\n",
    "indices = np.argsort(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a4999",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e12b0",
   "metadata": {},
   "source": [
    "- Lead Time , Market segment online and average price per room are still most imprtant features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-maine",
   "metadata": {
    "id": "obvious-maine"
   },
   "source": [
    "## Model Performance Comparison and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-kinase",
   "metadata": {
    "id": "everyday-kinase"
   },
   "outputs": [],
   "source": [
    "# training performance comparison\n",
    "\n",
    "models_train_comp_df = pd.concat(\n",
    "    [\n",
    "        decision_tree_perf_train.T,\n",
    "        decision_tree_tune_perf_train.T,\n",
    "        decision_tree_post_perf_train.T,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "models_train_comp_df.columns = [\n",
    "    \"Decision Tree sklearn\",\n",
    "    \"Decision Tree (Pre-Pruning)\",\n",
    "    \"Decision Tree (Post-Pruning)\",\n",
    "]\n",
    "print(\"Training performance comparison:\")\n",
    "models_train_comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test performance comparison\n",
    "\n",
    "models_test_comp_df = pd.concat(\n",
    "    [\n",
    "        decision_tree_perf_test.T,\n",
    "        decision_tree_tune_perf_test.T,\n",
    "        decision_tree_post_test.T,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "models_test_comp_df.columns = [\n",
    "    \"Decision Tree sklearn\",\n",
    "    \"Decision Tree (Pre-Pruning)\",\n",
    "    \"Decision Tree (Post-Pruning)\",\n",
    "]\n",
    "print(\"Test performance comparison:\")\n",
    "models_test_comp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220da356",
   "metadata": {},
   "source": [
    "- While the \"Decision Tree sklearn\" model has the best training performance, its performance on the validation dataset is not as strong, suggesting potential overfitting\n",
    "- Both pre-pruning and post-pruning techniques have led to models that exhibit better generalization to the validation data. The \"Decision Tree (Post-Pruning)\" model, which utilizes post-pruning, seems to strike a good balance between accuracy and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-retailer",
   "metadata": {
    "id": "nasty-retailer"
   },
   "source": [
    "## Actionable Insights and Recommendations\n",
    "\n",
    "- What profitable policies for cancellations and refunds can the hotel adopt?\n",
    "- What other recommedations would you suggest to the hotel?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf37ddb6",
   "metadata": {
    "id": "amino-prediction"
   },
   "source": [
    "## Profitable Policies for Cancellations and Refunds:\n",
    "\n",
    "- Flexible Cancellation Options: Implement a tiered approach to cancellation policies. Offer non-refundable rates at a lower price for guests who are certain of their plans and refundable rates at a slightly higher price for those seeking flexibility.\n",
    "\n",
    "- Last-Minute Cancellation Fees: Apply cancellation fees that increase as the arrival date approaches. This encourages guests to make decisions earlier, reducing last-minute cancellations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fabd97",
   "metadata": {},
   "source": [
    "## Recommendations for Handling Cancellations and Enhancing Revenue:\n",
    "\n",
    "- Personalized Offers: Leverage guest data to send personalized offers to guests with a higher risk of cancellation. This could incentivize them to retain their bookings.\n",
    "- Early Bird Specials: Offer exclusive discounts or perks for guests who book well in advance. This incentivizes guests to plan their trips earlier, resulting in longer lead times.\n",
    "- Dynamic Packages: Bundle value-added services (e.g., spa, restaurant discounts) with bookings at a reduced rate. This could make cancellations less appealing as guests would be forfeiting these benefits.\n",
    "- Feedback and Surveys: Collect guest feedback on reasons for booking early or late. Use this information to fine-tune your strategies and meet guest preferences.\n",
    "- Real-Time Monitoring: Implement real-time monitoring of booking patterns and trends. Adjust room rates dynamically to maximize revenue based on demand fluctuations."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "SLC_Project_Template_Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
